---
title: "Predicting readmission probability for diabetes inpatients"
graphics: yes
date: 'Due: Nov 13, 2016'
output:
  pdf_document:
    keep_tex: yes
    toc: yes
    toc_depth: 2
  html_document:
    number_sections: yes
    self_contained: no
    toc: yes
    toc_depth: 2
  word_document:
    toc: yes
    toc_depth: 2
subtitle: STAT 471/571/701, Fall 2016
header-includes:
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputnc}
- \pagestyle{fancy}
- \fancyfoot[CO,CE]{}
- \fancyfoot[LE,RO]{\thepage}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy=TRUE, fig.width=6,  fig.height=5, 
                      fig.align='left', dev = 'pdf')
```



### Packages
```{r}
# install.packages("ggplot2")
# install.packages("MASS")     # Many packages available. We only need a few.
library(MASS)  # load package MASS
library(lattice) # xyplot(), bwplot()
library(ggplot2)
library(ISLR)   # load data from ISLR
library(leaps)  # regsubsets()
library(car)
library(tree)
library(randomForest)
library(clusterGeneration)    # for plotting correlation
library(mnormt)      # for plotting correlation
library(corrplot)    # for ploting correlation
library(polycor)    # for correlation of categorical variables
library(glmnet)
library(stats)
library(SparseM)
```


### Analyzing Data in Raw form given in diabetic.data.csv and from website

```{r}
data_all <- read.csv("D:/courses_fall_2016/data mining stat571/mini project/diabetic.data.csv", header = TRUE)
str(data_all)
names(data_all)
#is.na(data_all)
```

### Analyzing data in filtered form given in readmission.csv

```{r}
data <- read.csv("D:/courses_fall_2016/data mining stat571/mini project/readmission.csv", header = TRUE)

str(data)
names(data)
sum(is.na(data))
sum(as.numeric(which(as.numeric(data$max_glu_serum)=="3")))
#we plot few variables against readmission indicator. Few variables like glucose serum test and A1c #test quantitatively gives patients seriousness for diabetes
# plot(data$max_glu_serum, data$A1Cresult,
#        pch  = 16,
#        cex  = 1.2,
#        col  = "blue",
#        xlab = "Glucose serum test",
#        ylab = "Readmission indicator",
#        main = "Readmission indicator vs Glucose serum test")
#Analyze readmission vs clinical test results
hist(as.numeric(data$readmitted), breaks=10, col="blue")
ggplot(data, aes(as.numeric(data$A1Cresult),fill=as.numeric(data$readmitted))) +
geom_histogram(bins = 60) +
xlab("Age (s)") + ylab("Count") +
ggtitle("Distribution of Age")
#Analyze readmission vs Race
hist(as.numeric(data$race), breaks=10, col="blue")
ggplot(data, aes(as.numeric(data$readmitted),fill=(data$race))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of Race in readmission")
# Analysis of readmission vs Gender
ggplot(data, aes(as.numeric(data$readmitted),fill=(data$gender))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of Gender in readmission")
#Conclusion: Almost equal males and females for readmission
#Analyze relation between readmission vs time spend in hospital
plot(data$time_in_hospital,jitter(as.numeric(data$readmitted)),pch=4)
#Conclusion-cant comment anything from result

# we cannot analyze each and every factor this way, therefore we analyze the ones which #we feel are more intuitive and then move on with building our model

# person's medical history can be a key indicator of present analysis.
# Therefore we try to relate those quantitities

# Analyze readmission vs number_inpatient
hist(as.numeric(data$number_inpatient), breaks=10, col="blue")
ggplot(data, aes(as.numeric(data$readmitted),fill=as.factor(data$number_inpatient))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of number_inpatient in readmission")
#Conclusion: No clear trend

# Analyze readmission vs number_outpatient
hist(as.numeric(data$number_outpatient), breaks=10, col="blue")
ggplot(data, aes(as.numeric(data$readmitted),fill=as.factor(data$number_outpatient))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of number_outpatient in readmission")
#Conclusion: No clear trend

# Analyze readmission vs number_emergency
hist(as.numeric(data$number_emergency), breaks=100, col="blue")
ggplot(data, aes(as.numeric(data$number_emergency),fill=as.factor(data$readmitted))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of number_emergency in readmission")
#Conclusion: No clear trend


```

We analyze two clinical results and we find that both of them have major proportion of "None" values and therefore they are inapt to provide any substantial information for our case.



### Model Selection
Many ways to select model:
1. Forward selection
2. Backward selection
3. Regsubset(Exhaustive)
4. LASSO/Elastic net
5. Random Forests
```{r}
#First we split that data into training and testing set. And we will not touch the #testing set except for the crossvalidation part
data_train <- data
# name.num <- sapply(data_train, is.numeric) # pulling out all the num. var's.
# pairs(data_train[name.num]) # pariwise scatter plots for num var's
# #above command will allow us to identify the correlation between any two numeric #variables
# cor(data.comp[name.num])
# pairs(data.comp[c("Salary","CHmRun", "Hits")], pch=16) #this command can be used to zoom in scatter plots for particular variables


#########VERY IMPORTANT: Is any transformation required on any of the variable?/######
##Find answer to the above question

## RSS solely cannot be used as a criteria to build our model. Presently we have 30 predictors + one output. If we use RSS as our criteria to build model, then we will build a model with 30 predictors as shown below

data_in_train <- data_train[, -31]   #variable "readmitted" removed from the predictor set
data_out_train <- data_train[,31]  #output variable

model_all_var <- lm(as.numeric(readmitted) ~ .,data=data_train)
summary(model_all_var)

## From the above summary, comment on some important predictors.
## Simply removing higher p-value variables does not give us correcct accurate model.

##Before going into model building, we need to efficiently deal with categorical variables. Work on this later.


#We will use following three parameters to judge the efficiency of model:
#1. Cp- indirect measurement of mean prediction error
#2. AIC- notion of likelihood function
#3. BIC- derived from Bayesian route, this criteria helps in selecting smaller models

############REGSUBSETS###########

### Exhaustive search- this method will give best model(one with least RSS) for particular number of predictors

#fit.exh <- regsubsets(as.numeric(readmitted) ~., data=data_train, nvmax=31, method="exhaustive")
# we see that exhaustive search method is extremely slow and it does throw and error
# therefore we search for best model using forward and backward selection method

### Forward selection
set.seed(123)
fit.forward <- regsubsets(as.numeric(readmitted) ~., data=data_train, nvmax=100, method="forward")
fit.forward
f.f <- summary(fit.forward)
f.f

# we use Cp, BIC and adjRsq criteria to find the optimum number of predictors

f.f$which
f.f$rsq
f.f$rss
f.f$bic



par(mfrow=c(3,1))
plot(f.f$adjr2 ,xlab =" Number of Variables ",
     ylab=" Adjusted RSq")
points(which.max(f.f$adjr2), f.f$adjr2[which.max(f.f$adjr2)], col ="red",cex =2, pch =20)

plot(f.f$cp ,xlab =" Number of Variables ",ylab="Cp")
points (which.min(f.f$cp), f.f$cp[which.min(f.f$cp)], col ="red",cex =2, pch =20)

plot(f.f$bic ,xlab=" Number of Variables ",ylab=" BIC")
points (which.min(f.f$bic), f.f$bic[which.min(f.f$bic)], col =" red",cex =2, pch =20)
par(mfrow=c(1,1))

##we get optimum number of predictors=43; using min BIC value

opt.size <- which.min(f.f$bic) # locate the optimal model size
opt.size
fit.exh.var <- f.f$which
fit.exh.var[opt.size,]  # this gives us the optimal variables selected

```

### Model selection using forward selection and for loop

We see that due to presence of too many categorical variables, we cannot select the model directly using regsubset function. We use for loop to find our model.

```{r}
# temp_mod=0
# aic_val=0
# for (i in 1:30){
#   temp_mod <- lm(as.numeric(readmitted) ~ data_train[,i], data=data_train)
#   aic_val[i]=AIC(temp_mod)
# }
# lm_input=0
# for (i in 1:10){
#   which.min(aic_val)
#   lm_input <- cbind(lm_input,data_train[,which.min(aic_val)])
#   aic_val[which.min(aic_val)] <- 0
# }
```

## Data Analysis

Complete data set given to us in file "diabetic.data.csv" has many redundant variables. Therefore we work with filtered dataset provided in file "readmission.csv" which contains only 31 variables. It has almost around 100000 data samples of different patients. We cannot work this large dataset because of the computational limit. The maximum number of trees formed in random forest is directly limited by the samples we take in training our model. More the samples, less number of trees can be formed owing to computational limit.


```{r}
set.seed(1)
data_t <- data[sample(nrow(data), 50000, replace=FALSE), ]
n=nrow(data)
test.index=sample(n, 40000)
length(test.index)
data_test=data[test.index,]
data_train=data[-test.index,]


## Cool plot for showing correlation between different predictors
M <- cor(data_train[sapply(data_train, is.numeric)])
corrplot(M, method="circle")
plot(data_train$gender,data_train$time_in_hospital)
# compared to male, female spends more time in hospital
plot(data_train$readmitted,data_train$time_in_hospital)
# on average, readmitted people tend to spend more time in hospital compared to other
# categories



##############################################################################
#to exploit the relation between two categorical variables, we create two way tables for
#various variables.

## 1. readmitted vs gender

with(data_train, table(readmitted, gender))

#from the table we see that, proportion of males and females being readmitted are almost same and thus gender does not give us any significant information

## 2. readmitted vs race

with(data_train, table(readmitted, race))
#from the table we see that race is one of the important factor. AfricanAmerican, Caucasian, Hispanic have higher proportion of readmissions.

##3. readmitted vs max_glu_serum

with(data_train, prop.table(table(readmitted, max_glu_serum),2))
with(data_train, addmargins(prop.table(table(readmitted, max_glu_serum),2)))

#glucose serum test gives us the sugar level of patient. This is one of the predictor which directly correlates with the severeness of disease. It can be seen from proportion table, that case with >200 and >300 serum levels have high probability of readmission compared to other cases.

##4. readmitted vs A1Cresult
with(data_train, addmargins(prop.table(table(readmitted, A1Cresult),2)))
# A1Ctest gives us the average glucose level in a person's body for past 3 months. Clearly this test would be a significant factor in determining readmission rate. But the results from conditional probability tables are counter intuitive. WHY?

##5. readmitted vs change
with(data_train, addmargins(prop.table(table(readmitted, change),2)))
#this variable shows us whether there was change in any of the medication of patient. Conditional probability table shows us that there is no significant impact, but patients whose medication are changes have slightly higher probability of readmission.

##6. readmitted vs diabetesmed
with(data_train, addmargins(prop.table(table(readmitted, diabetesMed),2)))
#this variable shows that there is higher probability of readmission given that diabetes medication is given to patient. This is one of the main factor which raises doubts on the previous treatment of patient. Also we see that this factor shows considerable difference between percent of patients readmitted within 30 days and after 30 days.

##7. readmitted vs disch_disp_modified
with(data_train, addmargins(prop.table(table(readmitted, disch_disp_modified),2)))
#There are four levels with this predictor:
#1. discharged to home
#2. discharged to home with home health service
#3. discharged/transferred to Skilled Nursing Facility
#4. other

# from the table, we see that people discharged to home have less probability of readmission. Patients who are provided with home health service or those who are transfered to SNF are more vulnerable to readmission. This is quite intuitive as patients not cured completely are the ones who need extra care. And eventually they are the ones who have high probability of readmission.

##8. readmitted vs adm_src_mod
with(data_train, addmargins(prop.table(table(readmitted, adm_src_mod),2)))
# from the table, we see that patients who are admitted to hospital on emergency basis or those who are transferred from home health serive have higher probability of readmission. Emergency case indicates that there is some severe malfunctioning with patient and it needs serious diagnosis. If this emergency is not well treated, then there is high probability of patient being readmitted. If the patient is transfered from home health service, then it is a sign of prolonged treatment, which in turn means that disease might be incurable and patient may be readmitted again. If the patient is admitted on the basis of physician referral, then it is quite possible that he is admitted for the first time and his disease is curable.


##9. readmitted vs adm_typ_mod
with(data_train, addmargins(prop.table(table(readmitted, adm_typ_mod),2)))
#no significant conclusion

##10. readmitted vs age_mod
with(data_train, addmargins(prop.table(table(readmitted, age_mod),2)))
#From the table we see that majority of people who are readmitted are the ones who have age greater than 20.

##11. readmitted vs diag1_mod
with(data_train, addmargins(prop.table(table(readmitted, diag1_mod),2)))
#diag1_mod gives us the ICD9 codes for primary treatment for various diseases. From the conditional probability table we see that this turns out to be one of the significant factor which differes for various different levels. For example, patient with ICD9 code equal to 250.6 has 21.6% probability of readmission compared to patient with ICD9 code equal to 996 which has only 5.17% probability of readmission. Analyzing this variable further, we see that 250.6 code corresponds to diabetes with neurological manifestations. Clearly this shows that patients who have undergone primary treatment for diabetes with neurological manifestations are far more vulnerabel to readmission within 30 days compared to other patients. Few other patients who have high probability of readmission within 30 days are:
# 1. Diabetes with other specifies manifestations
# 2. Disorders with fluid, electrolyte and acid-base imbalance
# 3. Septicaemia
# 4. Acute myocardial infarction
# 5. Other forms of chronic ischemic heart disease
# 6. Cardiac dysrhythmias

# Patients with least chances of readmission
# 1. Transient cerebral ischemia(435)
# 2. Asthma(493)
# 3. Lung disease(518)
# 4. Disorders of urethrea and urinary track(599)
# 5. cellulitis and abscess(682)
# 6. Dyspnea and respiratory abnormalities(786)
# 7. Fracture of neck of femur(820)
# 8. Complications of surgical and medical care(996)


##12. readmitted vs diag2_mod
with(data_train, addmargins(prop.table(table(readmitted, diag2_mod),2)))
# we see that patient who has undergone secondary treatment of Other cellulitis and abscess(682), has very high probability of readmission within 30 days.


##13. readmitted vs diag3_mod
with(data_train, addmargins(prop.table(table(readmitted, diag3_mod),2)))
# we see that patient who has undergone tertiary treatment for Alteration of consciousness(780), has very high probability of readmission within 30 days.


######VERY IMPORTANT CONCLUSION
# From the three level of treatment undergone by a patient we see that that patient with diabetes who has undergone primary treatment is very much likely for readmission within 30 days but this probability goes down considerably after secondary and tertiary treatment.


###effect of changes in various medications on readmission rate
with(data_train, addmargins(prop.table(table(readmitted, metformin),2)))
# when lowered, thre is high chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, glimepiride),2)))
# when lowered, there is chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, glipizide),2)))
# when raises, there is less chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, glyburide),2)))
# when raised, there is high chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, pioglitazone),2)))
# when lowered, thre is high chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, rosiglitazone),2)))
# when raised, there is high chance of readmission within 30 days
with(data_train, addmargins(prop.table(table(readmitted, insulin),2)))
# no significant change..WHY?


#########
##now we deal with effect of numerical variables on readmission rate
## 11. readmitted vs time_in_hospital
ggplot(data_train, aes(data_train$time_in_hospital,fill=as.factor(data_train$readmitted))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of number_emergency in readmission")

with(data_train, addmargins(prop.table(table(readmitted, time_in_hospital),2)))
#from the table it is clear that patients with more than one day of admit in hospitals are more likely to be readmitted within 30 days.

with(data_train, addmargins(prop.table(table(readmitted, num_lab_procedures),2)))
ggplot(data_train, aes(data_train$num_lab_procedures,fill=as.factor(data_train$readmitted))) +
geom_histogram(bins = 60) +
xlab("Readmission") + ylab("Count") +
ggtitle("Distribution of number_emergency in readmission")
cor(data_train$time_in_hospital,as.numeric(data_train$readmitted))
cor(data_train$num_lab_procedures,as.numeric(data_train$readmitted))
cor(data_train$num_procedures,as.numeric(data_train$readmitted))
cor(data_train$num_medications,as.numeric(data_train$readmitted))
cor(data_train$number_outpatient,as.numeric(data_train$readmitted))
cor(data_train$number_inpatient,as.numeric(data_train$readmitted))
cor(data_train$number_diagnoses,as.numeric(data_train$readmitted))
# All the above correlation value shows us that all numeric variables are important in predicting readmission probability.

#list of important predictorS:diabetesmed+diag1_mod+diag2_mod+diag3_mod+metformin+glimepiride+glipizide+glyburide+pioglitazone+rosiglitazone+time_in_hospital+num_lab_procedures+num_procedures+num_medications+number_outpatient+number_inpatient+number_diagnoses

```

We run the random forest algorithm for all the predictors variable. But we find that only few predictors are important and then we chose those predictors, build our random forest and compare the confusion matrix of this forest with the previous one. At this point, we would like to comment on few predictors which seems to be important through our Random Forest model.

## Random Forest

```{r}
# Mis-classification rates of OOB, misclassification errors for "0" and "1"
set.seed(1)
fit.rf <- randomForest(readmitted ~ ., data_train,mtry=4,ntree=40)
plot(fit.rf)
fit.rf.pred.y <- predict(fit.rf, data_test[,-c(31)])
mean(data_test[,c(31)] != fit.rf.pred.y)

# mean error is too high, therefore we filter important predictors

importance(fit.rf)  # this function returns us all the important variables.
varImpPlot(fit.rf,type=2)

# we now build a model with these important predictors

set.seed(1)
fit.rf_fin <- randomForest(readmitted ~ time_in_hospital + num_lab_procedures  + num_medications + num_procedures + number_inpatient + number_diagnoses + insulin + diag1_mod + diag2_mod + diag3_mod + disch_disp_modified + adm_src_mod + adm_typ_mod + age_mod, data_train, mtry=4, ntree=100)
fit.rf.pred.y <- predict(fit.rf_fin, data=data_test)
mean(data_test$readmitted != fit.rf.pred.y)
plot(fit.rf_fin)


set.seed(1)
fit.rf_fin <- randomForest(readmitted ~  diabetesMed+diag1_mod+diag2_mod+diag3_mod+metformin+glimepiride+glipizide+glyburide+pioglitazone+rosiglitazone+time_in_hospital+num_lab_procedures+num_procedures+num_medications+number_outpatient+number_inpatient+number_diagnoses, data_train, mtry=4, ntree=100)
fit.rf.pred.y <- predict(fit.rf_fin, data=data_test)
mean(data_test$readmitted != fit.rf.pred.y)
plot(fit.rf_fin)
```


## LASSO

```{r}
y <- data_train[, 31]
X <- model.matrix(readmitted~., data=data_train)[, -1]
set.seed(1)
fit.lasso.cv <- cv.glmnet(X, y, alpha=.99, family="multinomial")
plot(fit.lasso.cv)
fit.lasso.1se <- glmnet(X, y, alpha=0.99, family="multinomial", lambda=fit.lasso.cv$lambda.1se)
fit.lasso.1se.beta <- coef(fit.lasso.1se)
beta <- fit.lasso.1se.beta[which(fit.lasso.1se.beta !=0),] # non zero beta's
beta <- as.matrix(beta); beta
rownames(beta)
predict.lasso <- predict(fit.lasso.cv, as.matrix(data_test[, 31]), type = "class", s="lambda.1se")

# cant use logistic regression because it is valid only for binary classification, one option is to use multinom() function

#but we stick to decision trees and random forest to evaluate our final performance

#from lasso, we get following predictors as important ones
# (readmitted ~  diabetesMed+diag1_mod+diag2_mod+diag3_mod+num_medications+number_outpatient+number_inpatient+number_diagnoses+number_emergency+disch_disp_modified+adm_src_mod, data_train, mtry=4, ntree=100)

# we use these predictors to fit a random forest model
set.seed(1)
fit.rf_fin <- randomForest(readmitted ~  diabetesMed+diag1_mod+diag2_mod+diag3_mod+num_medications+number_outpatient+number_inpatient+number_diagnoses+number_emergency+disch_disp_modified+adm_src_mod, data_train, mtry=4, ntree=100)
fit.rf.pred.y <- predict(fit.rf_fin, data=data_test)
mean(data_test$readmitted != fit.rf.pred.y)
plot(fit.rf_fin)


##random forest does not give good accuracy, therefore we try to fit a single decision tree
set.seed(1)
fit.tree <- tree(readmitted ~  diabetesMed+diag1_mod+diag2_mod+diag3_mod+num_medications+number_outpatient+number_inpatient+number_diagnoses+number_emergency+disch_disp_modified+adm_src_mod, data_train)
fit.tree.pred.y <- predict(fit.tree,data=data_test)
mean(data_test$readmitted != fit.tree.pred.y)


```

## Forward selection method

```{r}
mydata$out <- relevel()
```